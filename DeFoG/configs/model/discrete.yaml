# Model settings
transition: 'marginal'                          # uniform, marginal, argmax, absorbfirst, absorbing
model: 'graph_tf'


extra_features: 'rrwp'        # 'all', 'cycles', 'eigenvalues', 'rrwp', 'rrwp_comp' or null
rrwp_steps: 12

# Do not set hidden_mlp_E, dim_ffE too high, computing large tensors on the edges is costly
hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 128}

# The dimensions should satisfy dx % n_head == 0
hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 128}

# training weight for edges, y, and nodes
lambda_train: [5, 0]              # X=1, E = lambda[0], y = lambda[1]

# General settings
name: 'graph-tf-model'      # Warning: 'debug' and 'test' are reserved name that have a special behavior

val_check_interval: null

log_every_steps: 50
number_chain_steps: 50        # Number of frames in each gif

# Test
generated_path: null

num_sample_fold: 1
evaluate_all_checkpoints: False

# Conditional Generation

target: 'k2'
guidance_weight: 2.0
# configs/experiment/aig.yaml
# @package _global_

general:

gpus: 1
wandb: 'online' # Or 'disabled' or 'offline'
abs_path_to_project_root: null # Set this if your project root is not parent of src, or let it be inferred

resume: null            # Path to checkpoint if resuming training
test_only: null         # Path to checkpoint for testing only

check_val_every_n_epochs: 100 # How often to run validation
sample_every_val: 1         # Run sampling every time validation runs

# Logging and Saving during training/validation
samples_to_generate: 64   # Number of graphs to generate during validation/sampling steps
samples_to_save: 16       # Number of generated graph visualizations to save
chains_to_save: 2         # Number of generation process GIFs to save

# For final evaluation after training or with test_only
final_model_samples_to_generate: 1000 # Generate more samples for robust final metrics
final_model_samples_to_save: 20
final_model_chains_to_save: 5
save_samples: True        # Save generated samples as files at the end of testing

# Conditional generation (set to True if your AIGs have labels you want to condition on)
conditional: False
# target: 'your_aig_label_key' # if conditional is True
# guidance_weight: 2.0          # if conditional is True (typical value from paper [cite: 1555])

train:
n_epochs: 20000             # Start with a moderate number, adjust based on convergence
batch_size: 64              # Adjust based on your AIG sizes and GPU memory
lr: 0.0002                  # Default learning rate
clip_grad: null             # Gradient clipping (e.g., 1.0) if needed
save_model: True            # As requested
ema_decay: 0.999            # Exponential Moving Average decay (0 means off)

# Time distortion for training (see DeFoG paper Sec 3.2, C.2 [cite: 972, 1373])
# 'polydec' often works well for structured data by focusing on later refinement steps [cite: 970, 1245]
time_distortion: 'polydec'

sample:
sample_steps: 1000          # Number of steps for the sampling process (denoising)



# Target guidance (omega) and Stochasticity (eta) (DeFoG paper Sec 3.2, B.2, B.3 [cite: 978, 1250, 986, 1260])
# Start with conservative values (0) and tune if needed.
# For Planar (structured, constraint-heavy), omega=0.05, eta=50 were used.
# AIGs are structured (DAGs), so some guidance/stochasticity might help.
omega: 0.0                  # Target guidance strength (0.0 to start, small positive if needed)
eta: 0.0                    # Stochasticity factor (0.0 to start, moderate positive if needed)

# Search flags (set to True to run hyperparameter search for sampling, then set best values above)
search: False               # e.g., 'all', 'target_guidance', 'distortion', 'stochasticity'

# Fixed sampling parameters (generally keep these defaults from model/discrete.yaml)
rdb: 'general'
rdb_crit: 'dummy'


# transition: 'marginal' # Default from model/discrete.yaml, generally a good start [cite: 1355, 1360]
                        # 'absorbfirst' was better for SBM[cite: 1367], but AIGs are different.
n_layers: 10                # Number of layers in the Graph Transformer

# Extra features (DeFoG paper Sec F.1, G.4 [cite: 1020, 1567, 1689])
# 'rrwp' (Relative Random Walk Probabilities) is generally beneficial for capturing structure.
#  extra_features: 'rrwp'
#  rrwp_steps: 16              # Number of steps for RRWP (12-20 is common)


dataset: aig # This will load configs/dataset/aig.yaml